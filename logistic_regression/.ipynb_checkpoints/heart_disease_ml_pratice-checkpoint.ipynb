{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1842a6d-1275-41bf-9153-a737db818f8b",
   "metadata": {},
   "source": [
    "**What is Logistic Regression?**\n",
    "\n",
    "> Logistic Regression is a **statistical model** that uses a **logistic (sigmoid) function** to estimate the **probability of a binary outcome**.  \n",
    "\n",
    "---\n",
    "\n",
    "**What does Binary Outcome mean?**\n",
    "\n",
    "A **binary outcome** means the **target variable** can take only **two possible values**, usually represented as `1` or `0`.\n",
    "\n",
    "- Example: *Does a patient have a disease?*  \n",
    "  - **Yes = 1**  \n",
    "  - **No = 0**\n",
    "\n",
    "We use `1` and `0` for simplicity in modeling:\n",
    "\n",
    "- **1 → Event happens (positive class)**  \n",
    "- **0 → Event does not happen (negative class)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46761cf0-ddb5-4c1e-bad4-8e86756e996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbff26-a5b0-4e9d-9b20-077733bd934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart_disease.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4ed5e-5b2c-4208-85f3-e183f8a74708",
   "metadata": {},
   "source": [
    "**Step 2: Perform Sanity Checks on the Dataset**\n",
    "\n",
    "Before building any model, we must **understand and inspect the dataset**.  \n",
    "This step ensures that the data is clean, consistent, and ready for analysis.\n",
    "\n",
    "**Common Sanity Checks:**\n",
    "1. **Check the shape** of the dataset → how many rows and columns.  \n",
    "2. **Look at the first few rows** to understand the structure.  \n",
    "3. **Check column data types** (numerical, categorical, object).  \n",
    "4. **Check for missing values**.  \n",
    "5. **Get summary statistics** (mean, median, min, max, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96267fe9-92a0-4a84-84dc-2cf681ff4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check shape attribute\n",
    "df.shape  # Tells the dimensions of the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06823d63-8e0c-4d8c-a858-d7a1d5054fce",
   "metadata": {},
   "source": [
    "So we have \n",
    " - 319,795 rows → these are the observations or records.\n",
    " - 18 columns → these are the features (variables) that describe each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8d6a8-34a1-44e0-beef-6357c68ec7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2) # Display first two rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827a751-aa6c-4ab6-8b4b-6fac4de00786",
   "metadata": {},
   "source": [
    "**Target Variable vs Features**\n",
    "\n",
    "- **HeartDisease** is our **target variable**  \n",
    "  - Also called the **dependent variable** or **label**  \n",
    "  - This is what we are trying to **predict**  \n",
    "  - It’s a **binary outcome**:  \n",
    "    - **Yes = 1**  \n",
    "    - **No = 0**\n",
    "\n",
    "---\n",
    "\n",
    "- All the other columns are **features**  \n",
    "  - Also called **independent variables** or **predictors**  \n",
    "  - These are the inputs the model will use to make predictions  \n",
    "  - Examples: `BMI`, `Smoking`, `Sex`, `AgeCategory`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de79ff8-72e7-4670-97ba-82afa7cd81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e47d3-cc7e-4cae-b627-3ba4b58d0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bb7a3-c63f-4f14-a7a3-e2603b388016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214ee2c-6533-4d32-9374-ac8c19d9dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() #Check how many duplicate rows exist in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d5b04-31f8-4f39-9ec0-358a72c0ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Target variable distribution\n",
    "df['HeartDisease'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed605df5-c638-4565-b0be-3a7ca387df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HeartDisease'].value_counts(normalize=True) # Check distribution in percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f117d64-2db9-4c77-ad15-d86b5572b36a",
   "metadata": {},
   "source": [
    "**Class Imbalance in the Target Variable**\n",
    "\n",
    "If we look at the distribution of the target variable:\n",
    "\n",
    "- Around **91% = No (0)**\n",
    "- Around **8.6% = Yes (1)**\n",
    "\n",
    "This is a **huge imbalance** in the data.\n",
    "\n",
    "Why is this important?  \n",
    "- A Logistic Regression model trained on this data might become **biased towards predicting \"No\"**.  \n",
    "- For example, if the model always predicts \"No\", it would already be **91% accurate**, but it would **completely fail to identify positive (Yes) cases**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed91a1a-c1ac-4150-a937-3ed725a40507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for outliers in BMI\n",
    "\n",
    "sns.boxplot(df['BMI'])\n",
    "plt.xlabel(\"Count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9cf33-ea75-49f5-90f2-9b9596ff7606",
   "metadata": {},
   "source": [
    "##### In the BMI column:\n",
    "  - Most values lie between **18 and 40**.\n",
    "  - Values above 40 are marked as **outliers**.\n",
    "  - Maximum BMI in the dataset is around **95**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbdb916-6c7b-4117-9341-f65d888c98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for outliers in Physical\n",
    "\n",
    "sns.boxplot(df['PhysicalHealth'])\n",
    "plt.xlabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d6621-8789-4d0b-8253-018dccfef374",
   "metadata": {},
   "source": [
    "#### Outliers in PhysicalHealth\n",
    "\n",
    "- Boxplot shows that most people reported **0 days** of poor physical health.\n",
    "- A small number reported values up to **30 days**.\n",
    "- Statistically, these appear as outliers, but they are **valid values** because:\n",
    "  - The feature is measured in days (range: 0–30).\n",
    "  - Reporting 30 days of poor health is possible, not an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d07dd3-4300-4d8d-a808-ef54311ad159",
   "metadata": {},
   "source": [
    "---\n",
    "#### Understanding the Box in a Boxplot\n",
    "\n",
    "- The **blue box** represents the **Interquartile Range (IQR)**, which contains the middle 50% of the data.\n",
    "- It has **three key horizontal lines**:\n",
    "\n",
    "1. **Bottom line of the box (Q1 / 25th percentile)**  \n",
    "   - 25% of the data lies **below** this value.  \n",
    "\n",
    "2. **Middle line of the box (Median / Q2 / 50th percentile)**  \n",
    "   - The midpoint of the data.  \n",
    "   - 50% of values lie **below** this line, 50% above.  \n",
    "\n",
    "3. **Top line of the box (Q3 / 75th percentile)**  \n",
    "   - 75% of the data lies **below** this value.  \n",
    "\n",
    "---\n",
    "\n",
    "### Whiskers\n",
    "- Lines extending out of the box = **whiskers**.  \n",
    "- They typically reach up to **1.5 × IQR** beyond Q1 and Q3.  \n",
    "- Points beyond the whiskers are plotted as **outliers**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd3675-6915-4e16-bd21-aa6c0e6cc945",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Cleaning\n",
    "\n",
    "**First, we clean the dataset to remove noise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9dd18-620b-4a6e-afab-9857def93306",
   "metadata": {},
   "source": [
    "---\n",
    " **1. Handle Missing values**\n",
    "\n",
    "  If any columns have missing data:\n",
    "  \n",
    " - Option 1: Drop rows/columns with too many missing values.\n",
    "     \n",
    " - Option 2: Impute (fill) them using mean/median for numeric or mode for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468cd517-09ac-4860-9cf2-efa5805c8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric and categorical\n",
    "df_num = df.select_dtypes(include=['float','int']).columns.tolist()\n",
    "\n",
    "df_cat = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns: \",df_num)\n",
    "\n",
    "print(f\"Categorical columns: \",df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b68b0e-ca5b-4932-b5bb-84578276d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing values \n",
    "df = df.dropna()\n",
    "\n",
    "# Impute numeric columns with their mean\n",
    "df[df_num] = df[df_num].fillna(df[df_num].mean())\n",
    "\n",
    "# Impute categorical columns with their mode\n",
    "for col in df_cat:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7cd10-8003-4a74-b49b-4f41e4164b5b",
   "metadata": {},
   "source": [
    "**2. Handle Duplicated Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8411b-2c24-43de-87f9-690fd0c4ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before Dropping duplicated rows:{df.duplicated().sum()}\")\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Before Dropping duplicated rows:{df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "**3. Handle outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231f417-0696-48e7-9a29-3c13af3442df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(feature):\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    print(outliers.shape[0])\n",
    "\n",
    "def cap_outliers(feature):\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    df[feature] = np.where(df[feature] < lower_bound, lower_bound,df[feature])\n",
    "    df[feature] = np.where(df[feature] > upper_bound, upper_bound, df[feature])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c1c4e-fe24-4c8c-88c5-53653a2a85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before Capping Outliers\")\n",
    "detect_outliers(\"BMI\")\n",
    "\n",
    "cap_outliers(\"BMI\")\n",
    "\n",
    "print(\"After Capping Outliers\")\n",
    "detect_outliers(\"BMI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4c64c-bdd7-4619-8f8c-73297d543044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb3638c8-f2bb-41b1-9b2f-f3c02824d5fd",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6997e-f468-459d-a345-e44e99511a9d",
   "metadata": {},
   "source": [
    "### 1. Univariate Analysis\n",
    "- Goal: Understand **what each feature looks like** individually.  \n",
    "- Tools:  \n",
    "  - For **numeric features**: Histograms, KDE plots, Boxplots, Summary statistics.  \n",
    "  - For **categorical features**: Count plots, Value counts, Bar plots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c13e8b-dd37-4550-a907-5d3d9613d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis of Numerical Features\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.histplot(df[\"BMI\"],kde=True )\n",
    "plt.title(\"Distribution of BMI\")\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4b31b-2676-4d6b-bb8e-728b86dd9fcf",
   "metadata": {},
   "source": [
    "### Distribution of BMI\n",
    "- Most BMI values are centered around **25–27**.  \n",
    "- The KDE curve shows a **right-skewed distribution** → some people have much higher BMI.  \n",
    "- Outliers above **40** are present but still valid (extremely high BMI cases).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034067d-3b20-4fd0-bdfe-7229d6c8ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis for Categorical features\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot(x=df[\"KidneyDisease\"],color='red')\n",
    "plt.xlabel(\"Kidney Disease\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Kidney Diseases\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818ec82-926c-430c-8573-af1db7f1e931",
   "metadata": {},
   "source": [
    "### Distribution of Kidney Diseases\n",
    "- The dataset is highly **imbalanced**:\n",
    "  - **No** → ~290,000 individuals  \n",
    "  - **Yes** → ~10,000 individuals  \n",
    "- Most people do **not** have kidney disease.  \n",
    "- Important: This imbalance may affect model training, similar to what we saw with `HeartDisease`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739858f-568d-4879-b9a3-7164d960d41b",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Bivariate Analysis\n",
    "- **Goal**: Understand relationships between two variables (especially with the target `HeartDisease`).  \n",
    "\n",
    "**1. Numeric vs Numeric**\n",
    "- Tools: Scatter plots, Correlation heatmaps.  \n",
    "- Example: Relationship between `BMI` and `SleepTime`.  \n",
    "\n",
    "**2. Numeric vs Categorical**\n",
    "- Tools: Boxplots, Violin plots.  \n",
    "- Example: Compare `BMI` distribution across HeartDisease = Yes/No.  \n",
    "\n",
    "**3. Categorical vs Categorical**\n",
    "- Tools: Countplots with `hue`, Crosstabs, Grouped bar plots.  \n",
    "- Example: Compare `Smoking` frequency across HeartDisease = Yes/No.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b365906-24db-40ea-8b05-1c43572ad1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical vs Categorical plot\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.countplot(x='Smoking', hue='HeartDisease', data=df)\n",
    "plt.title(\"Heart Disease Vs Smoking\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844d0c2-ce22-4927-bdd3-23c2ce537611",
   "metadata": {},
   "source": [
    "### 3. Correlation Analysis\n",
    "- Goal: Identify **multicollinearity** or strong relationships between numeric features.  \n",
    "- Tools: Correlation matrix, Heatmap.  \n",
    "- Example: Check correlation between `PhysicalHealth`, `MentalHealth`, and `SleepTime`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48d70a-631e-40b1-ba26-02ea677061bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df[df_num].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5ae15-a50e-48d3-a44d-6ef66c115ddb",
   "metadata": {},
   "source": [
    "### Correlation Heatmap (Numeric Features)\n",
    "\n",
    "- Shows pairwise correlation between numeric variables.  \n",
    "- Values range from **-1 to +1**:\n",
    "  - **+1** → perfect positive correlation  \n",
    "  - **-1** → perfect negative correlation  \n",
    "  - **0** → no correlation  \n",
    "\n",
    "Observations:\n",
    "- `PhysicalHealth` and `MentalHealth` have a **moderate positive correlation (~0.28)** → people reporting poor physical health often also report poor mental health.  \n",
    "- `BMI` has very weak correlation with other features.  \n",
    "- `SleepTime` has weak negative correlation with `MentalHealth (-0.12)`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68a54c-be89-4db5-8627-2752db645dca",
   "metadata": {},
   "source": [
    "### 4. Class Balance\n",
    "- Goal: Confirm **distribution of the target variable**.  \n",
    "- Already checked → Highly imbalanced (91% No, 9% Yes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afa5dc-77ad-4ed6-9c81-5490a52ced76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = df[\"HeartDisease\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9610b3e-4cea-4efd-9366-faf16bfd8651",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33292096-621e-4e57-9ade-36aa6ad230b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9e1ed-2514-40df-b4d1-d220fac96ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns = 'HeartDisease')\n",
    "y = df[\"HeartDisease\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "                                            X,y,\n",
    "                                            random_state=42, \n",
    "                                            test_size=0.2 ,\n",
    "                                            stratify=y\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43030cb9-ddfe-4ece-b97e-97df58ea4d6b",
   "metadata": {},
   "source": [
    "**Why Split Before Scaling/Encoding?**\n",
    "\n",
    "- To avoid **data leakage** → test set must remain unseen.  \n",
    "- To mimic **real-world use** → we train only on training data, then apply the same transformations on unseen data.  \n",
    "- To keep **consistency** → \n",
    "  - Training set: `fit_transform()`  \n",
    "  - Test set: `transform()`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3225f41-cdaf-406d-bcbd-a6ea0943636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Ensure target is not in categorical feature list\n",
    "if \"HeartDisease\" in df_cat:\n",
    "    df_cat.remove(\"HeartDisease\")\n",
    "\n",
    "# Scale numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_train[df_num] = scaler.fit_transform(X_train[df_num])\n",
    "X_test[df_num] = scaler.transform(X_test[df_num])\n",
    "\n",
    "# Encode categorical columns\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "X_train_encoded = pd.DataFrame(\n",
    "    encoder.fit_transform(X_train[df_cat]),\n",
    "    columns=encoder.get_feature_names_out(df_cat),\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_encoded = pd.DataFrame(\n",
    "    encoder.transform(X_test[df_cat]),\n",
    "    columns=encoder.get_feature_names_out(df_cat),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Replace categorical with encoded features\n",
    "X_train = X_train.drop(columns=df_cat).join(X_train_encoded)\n",
    "X_test = X_test.drop(columns=df_cat).join(X_test_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e7b01-ff5b-4b46-843c-652f982e5445",
   "metadata": {},
   "source": [
    "**Scale Numeric Features**\n",
    "\n",
    "   - Use `StandardScaler` → mean = 0, std = 1.\n",
    "   - Prevents large-scale variables from dominating.\n",
    "---\n",
    "\n",
    "**Encode Categorical Features**\n",
    "\n",
    "   - Use `OneHotEncoder`:\n",
    "     - `drop='first'` → avoid dummy variable trap.\n",
    "     - `sparse_output=False` → return dense DataFrame.\n",
    "     - `handle_unknown='ignore'` → safe for unseen categories in test.\n",
    "---\n",
    "**Combine Features**\n",
    "   - Drop original categorical columns.\n",
    "   - Join encoded columns back to the dataset.\n",
    "   - Now `X_train` and `X_test` are fully numeric → ready for Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8166ab-75a3-4ad7-9a5b-e5befd1673f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f92be-2110-49ec-b414-a4addfa13561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train model\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f76eee-079e-48a8-ad8e-eabdb638a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_pred_proba = log_reg.predict_proba(X_test)[:, 1]  # probabilities for ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee19f24-18c1-4ac0-8333-adea3806462d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05aef08-093a-41d5-8de4-2dc0aa742746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Precision, Recall, F1-score\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b81e0-0fcd-42bd-81dc-a7ad0905f706",
   "metadata": {},
   "source": [
    "**Key Insight**\n",
    "- High overall accuracy (91%) is misleading due to **class imbalance**.  \n",
    "- Model performs well at predicting **No**, but poorly at predicting **Yes** (low recall = 10%).  \n",
    "- ROC-AUC (0.83) shows the model has good discriminatory ability, but threshold tuning or resampling is needed to improve minority class detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f2ed0-3c4d-4189-8e66-f0ab5ecc93ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00cfe9-3a8f-43f4-ae81-7f458ea6f289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6131c9-39c5-4209-8ee1-38a1dcdd3932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42 , class_weight='balanced')\n",
    "\n",
    "# Train model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "                             \n",
    "\n",
    "# Predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_pred_proba = log_reg.predict_proba(X_test)[:, 1]  # probabilities for ROC-AUC\n",
    "\n",
    "# Precision, Recall, F1-score\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a594bf-0bc6-4508-82bf-7c626f491baf",
   "metadata": {},
   "source": [
    "#### Updated Model Performance (Logistic Regression with Class Weight/Resampling)\n",
    "\n",
    "- **Accuracy**: 74%  \n",
    "- **Classification Report**:\n",
    "  - Class **No** → Precision = 0.97, Recall = 0.74, F1 = 0.84  \n",
    "  - Class **Yes** → Precision = 0.23, Recall = 0.78, F1 = 0.35  \n",
    "\n",
    "### Key Insights\n",
    "- Accuracy dropped (from 91% → 74%) because the model is now focusing more on the minority class.  \n",
    "- **Recall for Yes improved significantly (10% → 78%)** → model is catching many more positive cases.  \n",
    "- Precision for Yes is lower (23%) → more false positives, but this trade-off is often acceptable in health-related predictions.  \n",
    "- **Balanced performance**: The model is no longer biased toward predicting \"No\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db47e69-c918-4dc3-bbbc-2d391356505b",
   "metadata": {},
   "source": [
    "---\n",
    "**Recall (Sensitivity / True Positive Rate)**\n",
    "- Definition: Out of all the **actual positive cases**, how many did the model correctly identify as positive?  \n",
    "\n",
    "**Precision (Positive Predictive Value)**\n",
    "- Definition: Out of all the cases the model **predicted as positive**, how many were actually positive?  \n",
    "\n",
    "\n",
    "**F1 is the **harmonic mean** of Precision and Recall.**\n",
    "\n",
    "- Why harmonic mean?\n",
    "  - It punishes extreme imbalance between Precision and Recall.\n",
    "  - Ensures a model must do well on **both** to get a good F1.\n",
    "- Useful for imbalanced datasets where Accuracy is misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9df629-96c7-4e46-9ac0-3bb0cf713b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3437429",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_list = df[df['sex'] == 1]\n",
    "female_list = df[df['sex'] == 0]\n",
    "\n",
    "print(\"Male List:\")\n",
    "display(male_list)\n",
    "\n",
    "print(\"Female List:\")\n",
    "display(female_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a45cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"heart_disease.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello Jupyter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576681c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028561a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
